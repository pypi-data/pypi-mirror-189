{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/centre-for-humanities-computing/conspiracies/blob/main/docs/tutorials/overview.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Coreference model\n",
    "A small use case of the coreference component in spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from conspiracies.coref import CoreferenceComponent \n",
    "\n",
    "nlp = spacy.blank(\"da\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe(\"allennlp_coref\")\n",
    "doc = nlp(\"Do you see Julie over there? She is really into programming!\")\n",
    "\n",
    "assert isinstance(doc._.coref_clusters, list)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    assert isinstance(sent._.coref_clusters, list)\n",
    "    assert isinstance(sent._.coref_clusters[0], tuple)\n",
    "    assert isinstance(sent._.coref_clusters[0][0], int)\n",
    "    assert isinstance(sent._.coref_clusters[0][1], Span)\n",
    "    sent._.resolve_coref # get resolved coref"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output a bit further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DOC LEVEL (Coref clusters)\")\n",
    "print(doc._.coref_clusters)\n",
    "print(\"-----\\n\\nSPAN LEVEL (sentences)\")\n",
    "for sent in doc.sents:\n",
    "    print(sent._.coref_clusters)\n",
    "print(\"-----\\n\\nSPAN LEVEL (entities)\\n\")\n",
    "for sent in doc.sents:\n",
    "    for i, coref_entity in sent._.coref_clusters:\n",
    "        print(f\"Coref Entity: {coref_entity} \\nAntecedent: {coref_entity._.antecedent}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headword Extraction\n",
    "A small use case of how to use the headword extraction component to extract headwords.\n",
    "\n",
    "````{note}\n",
    "For this example we will use the spacy pipeline `en_core_web_sm` if you don't have it installed you can do so by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "spacy download en_core_web_sm\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from conspiracies.HeadWordExtractionComponent import contains_ents\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"heads_extraction\", config={\"normalize_to_entity\": True, \"normalize_to_noun_chunk\": True})\n",
    "\n",
    "doc = nlp(\"Mette Frederiksen is the Danish politician.\")\n",
    "heads_spans = []\n",
    "\n",
    "print(doc._.most_common_ancestor)\n",
    "the_danish = doc[3:5]\n",
    "print(the_danish._.most_common_ancestor) # it normalizes to noun chunk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordpiece length normalization Extraction\n",
    "A small use case of how to use word piece length normalization to normalize the length of\n",
    "your texts in case you are applying transformer-based pipelines.\n",
    "\n",
    "````{note}\n",
    "For this example we will use the spacy pipeline `da_core_news_sm` if you don't have it installed you can do so by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "spacy download en_core_web_sm\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load nlp (we don't recommend a trf based spacy model as it is too slow)\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "# load huggingface tokenizer - should be the same as the model you wish to apply later\n",
    "tokenizer_name = \"alexandrainst/da-sentiment-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# An example with a very long text\n",
    "from conspiracies import wordpiece_length_normalization\n",
    "long_text = [\"Hej mit navn er Kenneth. \" * 200]\n",
    "normalized_text = wordpiece_length_normalization(long_text, nlp, tokenizer, max_length=500)\n",
    "normalized_text = list(normalized_text)\n",
    "assert len(normalized_text) > 1, \"a long text should be split into multiple texts\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RelationExtraction\n",
    "\n",
    "A Python library for extracting knowledge triplets from a text document.\n",
    "\n",
    "````{note}\n",
    "For this example we will use the spacy pipeline `en_core_web_sm` if you don't have it installed you can do so by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "spacy download en_core_web_sm\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: <CA752DFD-DB79-3AEF-B196-0DE84ACD1E36> /Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <598DDE6B-5A2E-3301-B9C5-9034AEC256A9> /Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: <CA752DFD-DB79-3AEF-B196-0DE84ACD1E36> /Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <598DDE6B-5A2E-3301-B9C5-9034AEC256A9> /Users/au561649/.virtualenvs/conspiracies/lib/python3.8/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/au561649/Github/conspiracies/src/conspiracies/relationextraction/other/bio.py:208: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  return [torch.Tensor(pred_tags) for pred_tags in total_pred_tags]\n",
      "/Users/au561649/Github/conspiracies/src/conspiracies/relationextraction/other/bio.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(res, dtype=torch.bool, device=tensor.device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m\"\u001b[39m\u001b[39mrelation_extractor\u001b[39m\u001b[39m\"\u001b[39m, config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m     25\u001b[0m pipe \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39mpipe(test_sents)\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m pipe:\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(d\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, d\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mrelation_triplets)\n",
      "File \u001b[0;32m~/.virtualenvs/conspiracies/lib/python3.8/site-packages/spacy/language.py:1575\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1573\u001b[0m     \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m pipes:\n\u001b[1;32m   1574\u001b[0m         docs \u001b[39m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1575\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n\u001b[1;32m   1576\u001b[0m     \u001b[39myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/.virtualenvs/conspiracies/lib/python3.8/site-packages/spacy/util.py:1598\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[1;32m   1591\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1592\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m   1596\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1597\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1598\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1599\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1600\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/Github/conspiracies/src/conspiracies/relationextraction/wrap_model_spacy.py:163\u001b[0m, in \u001b[0;36mSpacyRelationExtractor.pipe\u001b[0;34m(self, stream, batch_size)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m stream:\n\u001b[1;32m    162\u001b[0m     sents \u001b[39m=\u001b[39m [sent\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_annotations(doc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(sents))  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/Github/conspiracies/src/conspiracies/relationextraction/wrap_model_spacy.py:175\u001b[0m, in \u001b[0;36mSpacyRelationExtractor.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, docs: Iterable[Doc]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[1;32m    167\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline's model to a batch of docs, without modifying\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m    them.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/transformer#predict\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mextract_relations(docs)\n",
      "File \u001b[0;32m~/Github/conspiracies/src/conspiracies/relationextraction/knowledge_triplets.py:114\u001b[0m, in \u001b[0;36mKnowledgeTriplets.extract_relations\u001b[0;34m(self, text, verbose)\u001b[0m\n\u001b[1;32m    112\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    113\u001b[0m prepared_sent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_data(sents\u001b[39m=\u001b[39mtext)\n\u001b[0;32m--> 114\u001b[0m extractions \u001b[39m=\u001b[39m extract_to_dict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bert_model, prepared_sent)\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    116\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTIME: \u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n",
      "File \u001b[0;32m~/Github/conspiracies/src/conspiracies/relationextraction/extract.py:318\u001b[0m, in \u001b[0;36mextract_to_dict\u001b[0;34m(args, model, loader)\u001b[0m\n\u001b[1;32m    315\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mextraction_span\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m    316\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mextraction\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 318\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m    319\u001b[0m     token_strs \u001b[39m=\u001b[39m [[word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sent] \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39masarray(batch[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m])\u001b[39m.\u001b[39mT]\n\u001b[1;32m    320\u001b[0m     sentences \u001b[39m=\u001b[39m batch[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.virtualenvs/conspiracies/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.virtualenvs/conspiracies/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1196\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[39m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1196\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shutdown_workers()\n\u001b[1;32m   1197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \n\u001b[1;32m   1201\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/conspiracies/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1322\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1318\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     \u001b[39m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     \u001b[39m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m     w\u001b[39m.\u001b[39;49mjoin(timeout\u001b[39m=\u001b[39;49m_utils\u001b[39m.\u001b[39;49mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1323\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues:\n\u001b[1;32m   1324\u001b[0m     q\u001b[39m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/opt/homebrew/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_fork.py:44\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m wait\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinel], timeout):\n\u001b[1;32m     45\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/homebrew/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from conspiracies.relationextraction import SpacyRelationExtractor\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "test_sents = [\n",
    "    \"Pernille Blume vinder delt EM-sølv i Ungarn.\",\n",
    "    \"Pernille Blume blev nummer to ved EM på langbane i disciplinen 50 meter fri.\",\n",
    "    \"Hurtigst var til gengæld hollænderen Ranomi Kromowidjojo, der sikrede sig guldet i tiden 23,97 sekunder.\",\n",
    "    \"Og at formen er til en EM-sølvmedalje tegner godt, siger Pernille Blume med tanke på, at hun få uger siden var smittet med corona.\",\n",
    "    \"Ved EM tirsdag blev det ikke til medalje for den danske medley for mixede hold i 4 x 200 meter fri.\",\n",
    "    \"In a phone call on Monday, Mr. Biden warned Mr. Netanyahu that he could fend off criticism of the Gaza strikes for only so long, according to two people familiar with the call\",\n",
    "    \"That phone call and others since the fighting started last week reflect Mr. Biden and Mr. Netanyahu's complicated 40-year relationship.\",\n",
    "    \"Politiet skal etterforske Siv Jensen etter mulig smittevernsbrudd.\",\n",
    "    \"En av Belgiens mest framträdande virusexperter har flyttats med sin familj till skyddat boende efter hot från en beväpnad högerextremist.\",\n",
    "]\n",
    "\n",
    "\n",
    "# change these to your purposes. 2.7 is the default confidence threshold (the bulk of bad relations not kept and the majority of correct ones kept)\n",
    "# batch_size should be changed according to your device. Can most likely be bumped up a fair bit\n",
    "config = {\"confidence_threshold\": 2.7, \"model_args\": {\"batch_size\": 10}}\n",
    "nlp.add_pipe(\"relation_extractor\", config=config)\n",
    "\n",
    "pipe = nlp.pipe(test_sents)\n",
    "\n",
    "for d in pipe:\n",
    "    print(d.text, \"\\n\", d._.relation_triplets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation extraction without using SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conspiracies.relationextraction import KnowledgeTriplets\n",
    "\n",
    "\n",
    "test_sents = [\n",
    "    \"Lasse er en dreng på 26 år.\",\n",
    "    \"Jeg arbejder som tømrer\",\n",
    "    \"Albert var videnskabsmand og døde i 1921\",\n",
    "    \"Lasse lives in Denmark and owns two cats\",\n",
    "]\n",
    "\n",
    "\n",
    "test_sents = [\n",
    "    \"Pernille Blume vinder delt EM-sølv i Ungarn.\",\n",
    "    \"Pernille Blume blev nummer to ved EM på langbane i disciplinen 50 meter fri.\",\n",
    "    \"Hurtigst var til gengæld hollænderen Ranomi Kromowidjojo, der sikrede sig guldet i tiden 23,97 sekunder.\",\n",
    "    \"Og at formen er til en EM-sølvmedalje tegner godt, siger Pernille Blume med tanke på, at hun få uger siden var smittet med corona.\",\n",
    "    \"Ved EM tirsdag blev det ikke til medalje for den danske medley for mixede hold i 4 x 200 meter fri.\",\n",
    "    \"In a phone call on Monday, Mr. Biden warned Mr. Netanyahu that he could fend off criticism of the Gaza strikes for only so long, according to two people familiar with the call\",\n",
    "    \"That phone call and others since the fighting started last week reflect Mr. Biden and Mr. Netanyahu's complicated 40-year relationship.\",\n",
    "    \"Politiet skal etterforske Siv Jensen etter mulig smittevernsbrudd.\",\n",
    "    \"En av Belgiens mest framträdande virusexperter har flyttats med sin familj till skyddat boende efter hot från en beväpnad högerextremist.\",\n",
    "]\n",
    "\n",
    "\n",
    "# initialize a class object\n",
    "# call the class method for extracting triplets from a given list of sentences\n",
    "\n",
    "relations = KnowledgeTriplets()\n",
    "final_result = relations.extract_relations(test_sents)\n",
    "\n",
    "\n",
    "print(final_result[\"sentence\"])\n",
    "print(final_result[\"extraction_3\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conspiracies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e1b946b03001689b777b013a48d2e1f7160a2f7ba86ec293205a200812f9d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
