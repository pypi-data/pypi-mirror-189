# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['jobsworthy',
 'jobsworthy.model',
 'jobsworthy.observer',
 'jobsworthy.observer.domain',
 'jobsworthy.observer.repo',
 'jobsworthy.performance',
 'jobsworthy.performance.repo',
 'jobsworthy.repo',
 'jobsworthy.spark_job',
 'jobsworthy.structure',
 'jobsworthy.util']

package_data = \
{'': ['*']}

install_requires = \
['PyMonad>=2.4.0,<3.0.0',
 'azure-identity>=1.11.0,<2.0.0',
 'azure-storage-file-datalake>=12.9.1,<13.0.0',
 'delta-spark>=2.1.1,<3.0.0',
 'dependency-injector>=4.40.0,<5.0.0',
 'pino>=0.6.0,<0.7.0',
 'pyspark>=3.3.0,<4.0.0',
 'rdflib>=6.2.0,<7.0.0',
 'validators>=0.20.0,<0.21.0']

entry_points = \
{'console_scripts': ['infra = databricker.infra:cli']}

setup_kwargs = {
    'name': 'jobsworthy',
    'version': '0.6.2',
    'description': '',
    'long_description': '# Jobsworth\n\nA set of utility functions and classes to aid in build Spark jobs on Azure databricks.\n\n## Job Configuration\n\n## Spark Job Module\n\nJob provides a decorator which wraps the execution of a spark job. You use the decorator at the entry point for the job.\nAt the moment it performs 1 function; calling all the registered initialisers.\n\n```python\nfrom jobsworthy import spark_job\n```\n\n```python\n@spark_job.job()\ndef execute(args=None) -> monad.EitherMonad[value.JobState]:\n    pass\n```\n\nTo register initialisers (to be run just before the job function is called) do the following.\n\n```python\n@spark_job.register()\ndef some_initialiser():\n    ...\n```\n\nThe initialisers must be imported before the job function is called; to ensure they are registered. To do that, either\nimport them directly in the job module, or add them to a module `__init__.py` and import the module.\n\n## Model Module\n\n### Streamer\n\nThe `Streamer` module provides a fluent streaming abstraction on top of 2 hive repos (the from and to repos). The\nStreamer runs a pipeline as follows:\n\n+ Read from the source repo.\n+ Perform a transformation.\n+ Write to the target repo. This is where the stream starts and uses pyspark streaming to perform the read, transform\n  and write.\n+ Wait for the stream to finish.\n\nTo setup a stream:\n\n```python\nfrom jobsworthy import model\n\nstreamer = (model.STREAMER()\n            .stream_from(from_table)\n            .stream_to(to_table)\n            .with_transformer(transform_fn))\n\n# Execute the stream\nresult = streamer.run()\n\n# When successful it returns the Streamer wrapped in a Right.\n# When there is a failure, it returns the error (subtype of JobError) wrapped in a Left   \n\nassert result.is_right()\n```\n\nSome transformation functions require data from outside the input table. You can configure the streamer with additional\ntransformation context by passing in kwargs on the `with_transformer`.\n\n```python\nfrom dataclasses import dataclass\nfrom jobsworthy import model\n\n\n@dataclass\nclass TransformContext:\n    run_id: int\n\n\ndef transform_fn_with_ctx(df, **kwargs):\n    ...\n\n\nstreamer = (model.Streamer()\n            .stream_from(from_table)\n            .stream_to(to_table)\n            .with_transformer(transform_fn_with_ctx, run=TransformContext(run_id=1)))\n\n```\n\nWhen configuring the `stream_to` table, you can provide partition columns when writing the stream. Provide a tuple of\ncolumn names.\n\n```python\nstreamer = model.STREAMER().stream_to(to_table, (\'name\',))\n```\n\n### Repository Module\n\nThe repo library offers a number of simple abstractions for managing Databrick/Spark databases and tables. It is by no\nmeans an object-mapper. Rather its a few classes with some simple functions we have found useful when working with Hive\ntables.\n\n```python\nfrom jobsworthy import repo\n```\n\n### SparkDB\n\n`Db` is the base class representing a Hive Database. Once constructed it is provided to the hive table classes when they\nare constructed.\n\n`Db` takes a [spark session](#spark-session) and a [job config](#job-configuration).\n\n```python\ndb = repo.Db(session=spark_test_session.create_session(), config=job_config())\n```\n\nWhen intialised it checks that the database (defined in the config) exists and creates it if it doesn\'t.\n\n### Hive Table\n\nThe `HiveTable` class is an abstraction for a delta or hive table. Inheriting from this class provides a number of\nhelper and table management. It also provides common reading and writing functions (both stream and batch).\n\n#### Basic Configuration\n\nThe most basic Hive table can look like this.\n\n```python\nclass MyHiveTable(repo.HiveRepo):\n    table_name = "my_hive_table"\n\n\n# Initialise it with a SparkDb instance\ndb = repo.Db(session=spark_test_session.create_session(), job_config=job_cfg())\nmy_table = MyHiveTable(db=db)\n\n# Create a dataframe from the table\ndf = MyHiveTable().read()  # => pyspark.sql.dataframe.DataFrame\n\n# Append to the Table\ndf2 = db.session.read.json("tests/fixtures/table1_rows.json", multiLine=True, prefersDecimal=True)\nmy_table.write_append(df2)\n```\n\n#### Table Lifecycle Events\n\nBy default, tables are created when first written to (via an append, an upsert, or a streaming function). Usually tables\nare creates as "managed", meaning that the table is added to the catalogue and its associated files are managed as a\nunit (that is a drop_table() call will remove the catalogue entry and the files).\n\nTable creation can be forced if required, using the following functions:\n\n+ `create_as_unmanaged_delta_table`. Creates an unmanaged table at the location defined by the naming strategy.\n+ `create_as_managed_delta_table`. Creates a managed table based on the configuration of the catalogue.\n\nThese functions can be called directly. Alternatively, they can be called as part of a callback event. The require that\na schema be defined for the table.\n\n```python\nclass MyHiveTable(repo.HiveRepo):\n    table_name = "my_hive_table"\n    schema = T.StructType([T.StructField(\'id\', T.StringType(), True)])\n\n\nmy_table = MyHiveTable(db=db)\nmy_table.create_as_managed_delta_table()\n```\n\n#### Table Schema\n\n#### Partitioning and Pruning\n\n#### Table Properties\n\nHive table properties provide a means of storing key value pairs in the table, which are retrievable as a dataframe. The\ntable property key is a URN, while the value is a string. To store more complex objects in the value, requires\nserialisation into a string and interpretation outside the library.\n\nUse `repo.TableProperty` class to declare properties. This class takes a number of key formats; a URN, URN without the\nURN portion (as a shortcut) and a common table property using the repo.DataAgreementType enum.\n\nThe table properties are declared as a class property on the table. The repo module is then able to maintain those\nproperties on the table. The merging of properties is the responsibility of the table instance.\nCalling `self.property_manager.merge_table_properties()` will explicitly merge the declared difference of the properties\ndefined in the table with the properties on the Hive Table itself.\n\nWhen table properties are declared, and the Hive table is created (`create_as_unmanaged_delta_table()`) the table\nproeprties are merged to the table.\n\nThe table instance can also use callbacks to call the merge_table_properties() function.\n\n```python\n# Table using a URN (without the \'urn:\' prefix, which is added when merged to the table).\n# The properties are merged when the table instance is created.  This is an idempotent operation.\nclass MyHiveTable1(repo.HiveRepo):\n    table_name = "my_hive_table_3"\n\n    table_properties = [\n        repo.TableProperty("my_namespace:spark:table:schema:version", "0.0.1")\n    ]\n\n    def after_initialise(self, _result):\n        self.property_manager.merge_table_properties()\n\n\n# Table showing the use of a full URN, and the merge executed when the table is created.\nclass MyHiveTable2(repo.HiveRepo):\n    table_name = "my_hive_table_2"\n\n    table_properties = [\n        repo.TableProperty("urm:my_namespace:spark:table:schema:version", "0.0.1")\n    ]\n\n    def after_initialise(self, _result):\n        self.create_as_unmanaged_delta_table()\n\n\n# Table showing the use of a predefined property.  Note the need to provide the namespace to allow\n# the URN to be in the form of urn:<namespace>:<specific-part>\nclass MyHiveTable3(repo.HiveRepo):\n    table_name = "my_hive_table_3"\n\n    table_properties = [\n        repo.TableProperty(repo.DataAgreementType.SCHEMA_VERSION, "0.0.1", "my_namespace")\n    ]\n\n    def after_initialise(self, _result):\n        self.create_as_unmanaged_delta_table()\n```\n\nThere are a number of defined URNs that define specific data agreement properties. They are available in the\nENUM `repo.DataAgreementType`:\n\n+ `SCHEMA`.\n    + URN is `urn:{ns}:spark:table:schema`\n+ `SCHEMA_VERSION`.\n    + URN is `urn:{ns}:spark:table:schema:version`\n+ `PARTITION_COLUMNS`.\n    + URN is `urn:{ns}:spark:table:schema:partitionColumns`.\n    + Value is a comma separated list.\n+ `PRUNE_COLUMN`.\n    + URN is `urn:{ns}:spark:table:schema:pruneColumn`\n+ `DATA_PRODUCT`.\n    + URN is `urn:{ns}:dataProduct`\n+ `PORT`.\n    + URN is `urn:{ns}:dataProduct:port`\n+ `UPDATE_FREQUENCY`.\n    + URN is `urn:{ns}:dq:updateFrequency`\n+ `CATALOGUE`.\n    + URN is `urn:{ns}:catalogue`\n+ `DESCRIPTION`.\n    + URN is `urn:{ns}:catalogue:description`\n\n#### Callbacks\n\n`HiveTable` has a number of callback events which the table class can implement:\n\n+ `after_initialise`. Called after the `HiveTable` `__init__` function has completed.\n+ `after_append`. Called after the `write_append` function has completed.\n+ `after_upsert`. Called after `upsert` has completed.\n+ `after_stream_write_via_delta_upsert`. Called after `stream_write_via_delta_upsert`\n\nOne use of the callbacks is to create the table as a Hive table, or to update table properties.  `HiveTable` provides a\nfunction called `create_as_unmanaged_delta_table`. This function creates an unmanaged delta table based on a schema\nprovided by table class. the `after_initialise` callback can be used to ensure the table is created with the appropriate\nschema and properties before data is written to it.\n\n```python\ndb = repo.Db(session=spark_test_session.create_session(), job_config=job_cfg())\n\n\nclass MyHiveTable(repo.HiveRepo):\n    table_name = "my_hive_table"\n\n    table_properties = [\n        repo.TableProperty(repo.DataAgreementType.SCHEMA_VERSION, "0.0.1", "my_namespace")\n    ]\n\n    def after_initialise(self):\n        self.create_as_unmanaged_delta_table()\n\n    def schema_as_dict(self):\n        return {\'fields\': [\n            {\'metadata\': {}, \'name\': \'id\', \'nullable\': True, \'type\': \'string\'},\n            {\'metadata\': {}, \'name\': \'name\', \'nullable\': True, \'type\': \'string\'},\n            {\'metadata\': {}, \'name\': \'pythons\', \'nullable\': True, \'type\': {\n                \'containsNull\': True,\n                \'elementType\': {\'fields\': [\n                    {\'metadata\': {},\n                     \'name\': \'id\',\n                     \'nullable\': True,\n                     \'type\': \'string\'}],\n                    \'type\': \'struct\'},\n                \'type\': \'array\'}},\n            {\'metadata\': {}, \'name\': \'season\', \'nullable\': True, \'type\': \'string\'}], \'type\': \'struct\'}\n\n\nmy_table = MyHiveTable(\n    db=db)  # Executes the after_initialise callback with invokes the create_as_unmanaged_delta_table fn  \n```\n\n#### Write Functions\n\nA `HiveTable` supports the following write functions:\n+ `try_upsert`.  An `upsert` wrapped in a monadic try.  Returns a Option monad.\n+ `upsert`. Performs a delta table merge operation.\n+ `try_write_append`. A `write_append` wrapped in a monadic try.  Returns a Option monad.\n+ `write_append`.  Appends rows to an existing table, or creates a new table from the dataframe if the table doesn\'t exist.\n+ `try_write_stream`\n+ `write_stream_append`\n+ `try_stream_write_via_delta_upsert`\n+ `stream_write_via_delta_upsert`\n\n\n#### Write Options\n\nAll the write functions have the ability to add spark-type options.  Currently, the only option supported is `mergeSchema`.  Use the `Option` class to specify option requirements.\n\nOptions:\n+ `Option.MERGE_SCHEMA`.  Write option `(\'mergeSchema\', \'true\')`, Spark config setter `(\'spark.databricks.delta.schema.autoMerge.enabled\', \'true\')`.  This option is used when the input dataframe has a different schema to the table being written to.  This can occur when the table is created prior to the first dataframe being written, or when a new dataframe requiring schema evolution is written.  Note that the schema evolution process for streams (especially delta streams) is a little different from batch appends and upserts.  The streaming version has performed by setting a configuration on the spark session, while the batch version uses Spark write options.   \n\n```python\nappend_opts = [repo.Option.MERGE_SCHEMA]\n\n# append a dataframe and merge the schema if necessary.\nmy_table.write_append(df, append_opts)\n```\n\n### Util Module\n\n### Spark Session\n\n### Secrets\n\nThe Secrets module obtains secrets using the Databricks DBUtils secrets utility. The module acts as a wrapper for\nDButils. This allows for secrets to be mocked in tests without needing DBUtils. The CosmosDB repository is injected with\nthe secrets provider to enable secured access to CosmosDB.\n\nThe provider requires access to the Spark session when running on Databricks. However this is not required in test. You\nalso provide Secrets with a wrapper for DBUtils with also, optionally, takes a session. Both test and production\nwrappers are available in the `util.databricks` module.\n\n```python\nfrom jobsworthy.util import secrets, databricks\n\nprovider = secrets.Secrets(session=di_container.session,\n                           config=job_config(),\n                           secrets_provider=databricks.DatabricksUtilsWrapper())\n```\n\nThe default secret scope name is defined from the `JobConfig` properties; `domain_name` and `data_product_name`,\nseparated by a `.`. This can be overridden by defining the scope on the `Secrets` constructor, or on the call\nto `get_secret`. It looks like this on the constructor.\n\n```python\nprovider = secrets.Secrets(session=di_container.session,\n                           config=job_config(),\n                           secrets_provider=databricks.DatabricksUtilsWrapper(),\n                           default_scope_name="custom-scope-name")\n```\n\nGetting a secret.\n\n```python\nprovider.get_secret(secret_name="name-of-secret")  # returns an Either[secret-key]\n```\n\nSecrets is also able to return a `ClientCredential` using an Azure AD client credentials grant. The grant requires that\nclient id and secrets are obtainable via DBUtils through key-vault with the key names defined in `JobConfig` in the\nproperties `client_id_key` and `client_secret_key`\n\n```python\nprovider.client_credential_grant()  # returns an Either[ClientCredential]\n```\n\nTesting using secrets. DBUtils is not available as an open source project. When creating the secrets provider, you can\nprovide a DBUtils mock class which is available. On this class you can also construct valid keys to be used for test (if\nrequired; the mock returns a dummy key response to any generic lookup).\n\nThe example below also shows how to use a non-default scope on the `get_secrets` function.\n\n```python\nfrom jobsworthy.util import secrets, databricks\n\ntest_secrets = {"my_domain.my_data_product_name": {\'my_secret\': \'a-secret\'},\n                "alt_scope": {\'my_secret\': \'b-secret\'}}\n\nprovider = secrets.Secrets(\n    session=di_container.session,\n    config=job_config(),\n    secrets_provider=databricks.DatabricksUtilMockWrapper(spark_test_session.MockPySparkSession, test_secrets))\n\nprovider.get_secret(non_default_scope_name="alt_scope", secret_name="my_secret")\n```\n\n## Structure Model\n\nThe Structure module provides functions for building a more abstract definition of a Hive table schema and abstractions\nfor creating table, column and cell data which can be provided as the data argument when creating a dataframe.\n\n### Table Schema\n\nThere are a number of ways to create a Hive table schema:\n\n+ Create a dataframe from data where the schema can be inferred (e.g. a json or csv file), and write it to Hive.\n+ Explicitly provide a schema on a create dataframe.\n+ Create a Hive table using `pyspark.sql` `CREATE TABLE`, providing a schema.\n\nThe `Table` and `Column` classes abstract this schema creation. Creating the `StructType([])` for the table. In the end\nthe columns will come from `pyspark.sql.types`; for instance `StringType()`, `LongType()`, `ArrayType()`, `StructType()`\n, etc.\n\nThere are 2 modes for creating table schema; DSL or construction functions. Both require the use of a vocabulary mapping\ndictionary, which enables a domain-like term to be mapped to the column (or struct) term. A simple vocab might look like\nthis:\n\n```python\ndef vocab():\n    return {\n        "columns": {\n            "column1": {\n                "hasDataProductTerm": "column_one"\n            },\n            "column2": {\n                "hasDataProductTerm": "column_two",\n                "sub1": {\n                    "term": "sub_one"\n                },\n                "sub2": {\n                    "term": "sub_two"\n                }\n            }\n        }\n    }\n```\n\nThe structure of the vocab does not need to match the structure of the hive table column structure, but it can help with\nvisual mapping. In the vocab dict term mapping comes from using the dict key `hasDataProductTerm` or just `term`.\n\nAn example use of the vocab looks like this.\n\n```python\nfrom jobsworthy import structure as S\n\n# initialise a Table with the vocab\ntable = S.Table(vocab=vocab())\n\n# the vocab term "columns.column1", will result in a column named "column_one" \ntable.column_factory(vocab_term="columns.column1")\n```\n\nNote that a table definition is created using the `Table` class. The structure of the abstract table is based on this\nobject.\n\n#### Schema DSL\n\nThe DSL enables creating a table schema definition in a more fluid way, and inline.\n\nThe DSL provides the following commands:\n\n+ `column()`. initiates the creation of a new column.\n+ `struct(term, nullable)`. creates a column which will be a `StructType`. It is ended with the `end_struct`\n  command.  `struct` and `end_struct` can be nested.\n+ `string(term, nullable)`. creates a column of `StringType`, or defines a string within a struct.\n+ `decimal(term, decimal_type, nullable)`. creates a column of `DecimalType`, or defines a decimal within a struct.\n+ `long(term, nullable)`. creates a column of `LongType`, or defines a string within a struct.\n+ `array(term, scalar_type, nullable)`. creates an array column of of a basic type (such as a `StringType()`), or\n  defines an array within a struct.\n+ `array_struct(term, nullable)`. creates a column of `ArrayType` which contains a struct, or defines a struct array\n  within a struct.\n+ `end_struct`. signal to declare that the definition of a struct is completed.\n\nHere is an example uses the complete set of commands.\n\n```python\nfrom pyspark.sql.types import DecimalType, StringType\nfrom jobsworthy import structure as S\n\ntable = (S.Table(vocab=vocab())\n         .column()  # column1: string\n         .string("columns.column1", nullable=False)\n\n         .column()  # column 2 struct with strings\n         .struct("columns.column2", nullable=False)\n         .string("columns.column2.sub1", nullable=False)\n         .string("columns.column2.sub1", nullable=False)\n         .end_struct()\n\n         .column()  # column 3: decimal\n         .decimal("columns.column3", DecimalType(6, 3), nullable=False)\n\n         .column()  # column 4: long\n         .long("columns.column5", nullable=False)\n\n         .column()  # column 5: array of strings\n         .array("columns.column4", StringType, nullable=False)\n\n         .column()  # column 6: array of structs\n         .array_struct("columns.column6", nullable=True)\n         .long("columns.column6.sub1", nullable=False)\n         .string("columns.column6.sub1", nullable=False)\n         .array("columns.column6.sub3", StringType, nullable=False)\n         .end_struct()  # end column6\n\n         .column()  # struct with strings and array of structs\n         .struct("columns.column7", nullable=False)\n         .string("columns.column7.sub1")\n\n         .struct("columns.column7.sub2", nullable=False)  # struct nested in a struct\n         .string("columns.column7.sub2.sub2-1")\n         .string("columns.column7.sub2.sub2-2")\n         .end_struct()\n\n         .end_struct()  # end column7\n         )\n```\n\nNote that a similar chained struct creation of a schema can also be achieved through the pyspark.sql.types module using\nthe `add()` function as the following example shows.\n\n```python\nfrom pyspark.sql import types as T\n\n(T.StructType()\n .add("column_one", T.StringType(), False)\n .add(\'column_two\', T.StructType().add("sub_two_one", T.StringType())))\n```\n\nWhich is the same as:\n\n```python\nfrom pyspark.sql import types as T\n\n(T.StructType([\n    T.StructField("column_one", T.StringType(), False),\n    T.StructField("column_one", T.StructType().add("sub_two_one", T.StringType()))\n]))\n```\n\n',
    'author': 'Col Perks',
    'author_email': 'wild.fauve@gmail.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/wildfauve/jobsworth',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'entry_points': entry_points,
    'python_requires': '>=3.9,<4.0',
}


setup(**setup_kwargs)
