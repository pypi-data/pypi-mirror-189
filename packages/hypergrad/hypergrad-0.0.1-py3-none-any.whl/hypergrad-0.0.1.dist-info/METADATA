Metadata-Version: 2.1
Name: hypergrad
Version: 0.0.1
Summary: Simple and extensible hypergradient for PyTorch
Project-URL: Documentation, https://github.com/moskomule/hypergrad
Project-URL: Issues, https://github.com/moskomule/hypergrad/issues
Project-URL: Source, https://github.com/moskomule/hypergrad
Author-email: Ryuichiro Hataya <hataya@nlab.ci.i.u-tokyo.ac.jp>
License-Expression: MIT
License-File: LICENSE.txt
Classifier: Development Status :: 4 - Beta
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: Implementation :: CPython
Requires-Python: >=3.10
Requires-Dist: rich
Requires-Dist: torch
Requires-Dist: torchvision
Description-Content-Type: text/markdown

# [hypergrad](mosko.tokyo/hypergrad) ![pytest](https://github.com/moskomule/hypergrad/workflows/pytest/badge.svg)

Simple and extensible hypergradient for PyTorch

<!-- [![PyPI - Version](https://img.shields.io/pypi/v/hypergrad.svg)](https://pypi.org/project/hypergrad)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/hypergrad.svg)](https://pypi.org/project/hypergrad) -->

## Installation

First, install `torch` and its accompanying `torchvision` appropriately. Then,

```console
pip install hypergrad
```

## Methods

### Implicit hypergradient approximation (via approximated inverse Hessian-vector product)

* conjugate gradient
* [Neumann-series approximation](https://proceedings.mlr.press/v108/lorraine20a.html)
* [Nystr√∂m method](to_be_updated)

Implementation of these methods can be found in `hypergrad/approximate_ihvp.py`

## Citation

To cite this repository,

```bibtex
@software{hypergrad,
    author = {Ryuichiro Hataya},
    title = {{hypergrad}},
    url = {https://github.com/moskomule/hypergrad},
    year = {2023}
}
```

`hypergrad` is developed as a part of the following research projects:

```bibtex
@inproceedings{hataya2023nystrom,
    author = {Ryuichiro Hataya and Makoto Yamada},
    title = {{Nystr\"om Method for Accurate and Scalable Implicit Differentiation}},
    booktitle = {AISTATS},
    year = {2023}
}
```